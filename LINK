
batch gradient descent code(also stochastic):
1.这个是按照公式求的，最后求的是loss（cost）fuction值。注意是h-y
http://daweibalong.iteye.com/blog/1660169
2.这个博主的是错的，楼11是对的。区别1，这个是用中间缓存了theta，比1，少一层循环。
http://blog.csdn.net/pennyliang/article/details/6998517

for(int i = 0; i < 1000 && loss > 0.0001; ++i) 
{ 
	float error = 0.0; 
	float cost[2] = {0.0, 0.0};

	for(int j = 0; j < 4; ++j) 
	{ 
		float h = 0.0; 
		for(int k = 0; k < 2; ++k) 
		{ 
			h += matrix[j][k] * theta[k]; 
		} 
		error = result[j] - h;      //这是 y-h，所以下面更新theta时是 + ！

		for(int k = 0; k < 2; ++k) 
		{ 
			cost[k] += error * matrix[j][k];
		} 
	}

	for(int k = 0; k < 2; ++k) 
	{ 
		theta[k] += learning_rate * cost[k]/4; // wi = wi + 1/m * sum((yi-hi)xj)
	}
}

3.python scikit-learning 应用！
http://www.bogotobogo.com/python/python_numpy_batch_gradient_descent_algorithm.php


ML class & note pdf:
http://cs229.stanford.edu/materials.html


http://www.cnblogs.com/superhuake/archive/2012/07/25/2609124.html
http://shiyanjun.cn/archives/744.html
http://www.52ml.net/15084.html


推荐：(IBM mahout文档)
http://www.zhihu.com/question/20558659

协同过滤:
http://www.zhihu.com/question/22404652

Pearson：
http://blog.csdn.net/database_zbye/article/details/8664516
pearson correlation 缺点 (G)


小教官
http://blog.tomtung.com/2007/10/noip05-fire/

菱形距离
http://blog.csdn.net/nys001/article/details/12637201




性能调优工具 Flame Graph：
http://structureddata.org/2012/06/18/linux-6-transparent-huge-pages-and-hadoop-workloads/


大话数据挖掘
http://blog.sina.com.cn/s/blog_6255c70101019ks3.html
hbase 性能调优
http://wangneng-168.iteye.com/blog/2067741

决策树 剪枝：
http://isilic.iteye.com/blog/1846726



大数据算法：
http://www.icourse163.org/course/hit-10001?tid=9003#/info


spark走读：
http://www.cnblogs.com/hseagle/category/569175.html
http://www.iteblog.com/archives/1181

小象学院：
http://www.chinahadoop.cn/login


ML 基础：
www.cnblogs.com/tornadomeet/
www.cnblogs.com/tornadomeet/archive/2012/03/21/2409421.html
www.cnblogs.com/tornadomeet/p/3395593.html


baidu 机器学习：
wenku.baidu.com/course/view/49e8b8f67c1cfad6195fa705


算法基础：
http://www.cnblogs.com/luweiseu/archive/2012/07/14/2591446.html


Forward-backward 算法
http://www.cnblogs.com/zhangchaoyang/articles/2219571.html
blog.sina.com.cn/s/blog_623c00930100haz6.html

scala 学习：
https://github.com/alexandru/scala-best-practices/


